using Microsoft.AspNetCore.Authorization;
using Microsoft.AspNetCore.Mvc;
using MyApi.Modules.AiChat.DTOs;
using MyApi.Modules.AiChat.Services;
using System.Security.Claims;

namespace MyApi.Modules.AiChat.Controllers
{
    /// <summary>
    /// Endpoint to generate AI responses via the local Ollama LLM.
    /// POST /api/GenerateWish ‚Äî non-streaming JSON response
    /// POST /api/GenerateWish/stream ‚Äî SSE streaming response
    /// AllowAnonymous: allows calls without JWT for development/testing.
    /// </summary>
    [ApiController]
    [Route("api/[controller]")]
    public class GenerateWishController : ControllerBase
    {
        private readonly IOllamaService _ollamaService;
        private readonly ILogger<GenerateWishController> _logger;

        public GenerateWishController(IOllamaService ollamaService, ILogger<GenerateWishController> logger)
        {
            _ollamaService = ollamaService;
            _logger = logger;
        }

        /// <summary>
        /// Generate AI response (non-streaming). Accepts prompt or messages[].
        /// </summary>
        [HttpPost]
        [AllowAnonymous]
        [ProducesResponseType(typeof(GenerateWishResponseDto), 200)]
        [ProducesResponseType(400)]
        [ProducesResponseType(500)]
        [ProducesResponseType(503)]
        public async Task<ActionResult<GenerateWishResponseDto>> Generate([FromBody] GenerateWishRequestDto dto)
        {
            var userId = GetCurrentUserId();
            _logger.LogInformation("üì• [GenerateWish] Incoming request ‚Äî user={UserId}, hasPrompt={HasPrompt}, msgCount={MsgCount}, model={Model}, temp={Temp}, maxTokens={MaxTokens}, stream={Stream}",
                userId,
                !string.IsNullOrWhiteSpace(dto.Prompt),
                dto.Messages?.Count ?? 0,
                dto.Model ?? "(default)",
                dto.Temperature?.ToString() ?? "(default)",
                dto.MaxTokens?.ToString() ?? "(default)",
                dto.Stream);

            if (!ModelState.IsValid)
            {
                _logger.LogWarning("‚ö†Ô∏è [GenerateWish] Invalid model state ‚Äî user={UserId}, errors={Errors}",
                    userId, string.Join("; ", ModelState.Values.SelectMany(v => v.Errors).Select(e => e.ErrorMessage)));
                return BadRequest(ModelState);
            }

            if (string.IsNullOrWhiteSpace(dto.Prompt) && (dto.Messages == null || dto.Messages.Count == 0))
            {
                _logger.LogWarning("‚ö†Ô∏è [GenerateWish] Empty request ‚Äî no prompt and no messages ‚Äî user={UserId}", userId);
                return BadRequest(new GenerateWishResponseDto
                {
                    Success = false,
                    Error = "Either 'prompt' or 'messages' must be provided"
                });
            }

            // Log message contents for debugging
            if (dto.Messages != null)
            {
                for (int i = 0; i < dto.Messages.Count; i++)
                {
                    var msg = dto.Messages[i];
                    _logger.LogDebug("üìù [GenerateWish] Message[{Index}] role={Role}, contentLen={Len}, preview=\"{Preview}\"",
                        i, msg.Role, msg.Content?.Length ?? 0, msg.Content?.Substring(0, Math.Min(msg.Content.Length, 120)) ?? "(null)");
                }
            }

            try
            {
                _logger.LogInformation("üöÄ [GenerateWish] Calling OllamaService.GenerateAsync ‚Äî user={UserId}", userId);
                var result = await _ollamaService.GenerateAsync(dto, userId);

                _logger.LogInformation("üì§ [GenerateWish] Result ‚Äî user={UserId}, success={Success}, model={Model}, responseLen={Len}, durationNs={DurationNs}, error={Error}",
                    userId, result.Success, result.Model, result.Response?.Length ?? 0, result.TotalDurationNs, result.Error ?? "(none)");

                if (!result.Success)
                {
                    if (result.Error?.Contains("Cannot reach") == true ||
                        result.Error?.Contains("timed out") == true)
                    {
                        _logger.LogWarning("üîå [GenerateWish] Service unavailable ‚Äî user={UserId}, error={Error}", userId, result.Error);
                        return StatusCode(503, result);
                    }
                    _logger.LogWarning("‚ùå [GenerateWish] LLM error ‚Äî user={UserId}, error={Error}", userId, result.Error);
                    return StatusCode(502, result);
                }

                return Ok(result);
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "üí• [GenerateWish] Unhandled exception ‚Äî user={UserId}, exType={ExType}", userId, ex.GetType().Name);
                return StatusCode(500, new GenerateWishResponseDto
                {
                    Success = false,
                    Error = "Internal server error"
                });
            }
        }

        /// <summary>
        /// Stream AI response via SSE. Accepts prompt or messages[].
        /// </summary>
        [HttpPost("stream")]
        [AllowAnonymous]
        public async Task Stream([FromBody] GenerateWishRequestDto dto)
        {
            var userId = GetCurrentUserId();
            _logger.LogInformation("üì• [StreamChat] Incoming stream request ‚Äî user={UserId}, hasPrompt={HasPrompt}, msgCount={MsgCount}, model={Model}, temp={Temp}, maxTokens={MaxTokens}",
                userId,
                !string.IsNullOrWhiteSpace(dto.Prompt),
                dto.Messages?.Count ?? 0,
                dto.Model ?? "(default)",
                dto.Temperature?.ToString() ?? "(default)",
                dto.MaxTokens?.ToString() ?? "(default)");

            if (string.IsNullOrWhiteSpace(dto.Prompt) && (dto.Messages == null || dto.Messages.Count == 0))
            {
                _logger.LogWarning("‚ö†Ô∏è [StreamChat] Empty request ‚Äî no prompt and no messages ‚Äî user={UserId}", userId);
                Response.StatusCode = 400;
                await Response.WriteAsync("{\"error\":\"Either 'prompt' or 'messages' must be provided\"}");
                return;
            }

            // Log message contents for debugging
            if (dto.Messages != null)
            {
                for (int i = 0; i < dto.Messages.Count; i++)
                {
                    var msg = dto.Messages[i];
                    _logger.LogDebug("üìù [StreamChat] Message[{Index}] role={Role}, contentLen={Len}, preview=\"{Preview}\"",
                        i, msg.Role, msg.Content?.Length ?? 0, msg.Content?.Substring(0, Math.Min(msg.Content.Length, 120)) ?? "(null)");
                }
            }

            Response.ContentType = "text/event-stream";
            Response.Headers["Cache-Control"] = "no-cache";
            Response.Headers["Connection"] = "keep-alive";

            _logger.LogInformation("üöÄ [StreamChat] Starting SSE stream ‚Äî user={UserId}", userId);
            await _ollamaService.StreamChatAsync(dto, userId, Response.Body, HttpContext.RequestAborted);
            _logger.LogInformation("üèÅ [StreamChat] SSE stream ended ‚Äî user={UserId}", userId);
        }

        private string GetCurrentUserId()
        {
            return User.FindFirst(ClaimTypes.NameIdentifier)?.Value ??
                   User.FindFirst(ClaimTypes.Email)?.Value ??
                   User.FindFirst("sub")?.Value ??
                   "anonymous";
        }
    }
}
